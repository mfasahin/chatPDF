import streamlit as st
from PyPDF2 import PdfReader
import os
from dotenv import load_dotenv
import hashlib
from pathlib import Path

# LangChain
from langchain_groq import ChatGroq
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser


# ----------------- YARDIMCI FONKSÄ°YON -----------------
def get_pdf_hash(pdf_file):
    pdf_bytes = pdf_file.getvalue()
    return hashlib.md5(pdf_bytes).hexdigest()


# ----------------- CACHE: EMBEDDINGS -----------------
@st.cache_resource
def load_embeddings():
    # Kalite iÃ§in en stabil embedding modeli
    return HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )


# ----------------- SAYFA AYARLARI -----------------
st.set_page_config(page_title="PDF AsistanÄ±", page_icon="ğŸ¤–")
st.header("ğŸ¤– PDF Dosyanla Sohbet Et (Groq + Llama 3.3)")

# ----------------- API -----------------
load_dotenv()
api_key = os.getenv("GROQ_API_KEY")

if not api_key:
    st.error("âŒ API AnahtarÄ± BulunamadÄ±!")
    st.stop()

# ----------------- SIDEBAR -----------------
with st.sidebar:
    st.title("ğŸ“‚ PDF YÃ¼kle")
    pdf_dosyasi = st.file_uploader("DosyanÄ± buraya bÄ±rak", type="pdf")
    st.write("---")
    st.write("Profil: Kalite > HÄ±z")
    st.write("Model: Llama-3.3-70B")

# ----------------- ANA AKIÅ -----------------
if pdf_dosyasi is not None:
    # A) PDF OKUMA
    reader = PdfReader(pdf_dosyasi)
    metin = ""

    for sayfa in reader.pages:
        yazi = sayfa.extract_text()
        if yazi:
            metin += yazi

    if not metin.strip():
        st.error("âš ï¸ Bu PDF'ten metin okunamadÄ±!")
        st.stop()

    st.success(f"âœ… Dosya analiz edildi! ({len(metin)} karakter)")

    # B) METNÄ° PARÃ‡ALA
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    chunks = splitter.split_text(metin)

    # C) EMBEDDINGS (CACHEâ€™LÄ°)
    embeddings = load_embeddings()

    # ----------------- FAISS CACHE -----------------
    CACHE_DIR = Path("faiss_cache")
    CACHE_DIR.mkdir(exist_ok=True)

    pdf_hash = get_pdf_hash(pdf_dosyasi)
    faiss_path = CACHE_DIR / pdf_hash

    if faiss_path.exists():
        st.info("ğŸ“¦ Ã–nceden iÅŸlenmiÅŸ PDF bulundu, cache kullanÄ±lÄ±yor...")
        vectorstore = FAISS.load_local(
            faiss_path,
            embeddings,
            allow_dangerous_deserialization=True
        )
    else:
        st.info("ğŸ§  PDF ilk kez iÅŸleniyor, yÃ¼ksek kaliteli embedding oluÅŸturuluyor...")
        vectorstore = FAISS.from_texts(chunks, embeddings)
        vectorstore.save_local(faiss_path)

    # D) SORU
    st.write("---")
    soru = st.text_input(
        "Bu dokÃ¼man hakkÄ±nda ne bilmek istiyorsun?",
        placeholder="Ã–rn: Beowulf'taki pagan Ã¶geler nelerdir?"
    )

    if soru:
        with st.spinner("ğŸ§  Derinlemesine analiz ediliyor..."):
            # ğŸ”¥ KALÄ°TE ODAKLI LLM
            llm = ChatGroq(
                model_name="llama-3.3-70b-versatile",  # KALÄ°TE
                groq_api_key=api_key,
                temperature=0.2,  # daha tutarlÄ± cevaplar
                max_tokens=1024   # uzun ve detaylÄ± cevaplar
            )

            # ğŸ¯ KALÄ°TE ODAKLI PROMPT
            prompt = ChatPromptTemplate.from_template("""
AÅŸaÄŸÄ±daki baÄŸlamÄ± dikkatlice analiz ederek soruyu cevapla.
CevabÄ±nÄ± yalnÄ±zca verilen baÄŸlama dayandÄ±r.
BaÄŸlamda yoksa aÃ§Ä±kÃ§a "Bu dokÃ¼manda buna dair bilgi yok." de.

BaÄŸlam:
{context}

Soru:
{question}

DetaylÄ± ve tutarlÄ± bir cevap ver:
""")

            # ğŸ” Daha fazla baÄŸlam â†’ kalite â†‘
            retriever = vectorstore.as_retriever(
                search_kwargs={"k": 5}
            )

            chain = (
                {
                    "context": retriever,
                    "question": RunnablePassthrough()
                }
                | prompt
                | llm
                | StrOutputParser()
            )

            cevap = chain.invoke(soru)

            st.markdown("### ğŸ¤– Cevap:")
            st.write(cevap)
