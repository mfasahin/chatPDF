import streamlit as st
from prompts.qa_prompt import QA_PROMPT


def render_chat(llm, retriever, pdf_just_loaded=False):
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    
    # Ä°lk aÃ§Ä±lÄ±ÅŸ - PDF yÃ¼klenmeden
    if llm is None and "initial_greeting" not in st.session_state:
        welcome_msg = """
Merhaba! ğŸ‘‹ Ben senin PDF asistanÄ±nÄ±m.

BaÅŸlamak iÃ§in soldaki menÃ¼den bir PDF dosyasÄ± yÃ¼kle, ben onu analiz edeyim. Sonra iÃ§eriÄŸi hakkÄ±nda istediÄŸin sorularÄ± sorabilirsin! 

Ne tÃ¼r sorular sorabileceÄŸini merak ediyorsan:
- ğŸ“„ "Bu belge ne hakkÄ±nda?"
- ğŸ” "X konusuyla ilgili ne diyor?"
- ğŸ“Š "Ã–nemli noktalarÄ± Ã¶zetle"
- ğŸ’¡ "Y hakkÄ±nda detaylÄ± bilgi ver"

Hadi, PDF'ini yÃ¼kle ve baÅŸlayalÄ±m! ğŸš€
        """
        st.session_state.chat_history.append(("assistant", welcome_msg.strip()))
        st.session_state.initial_greeting = True
    
    # PDF yeni yÃ¼klendi
    elif pdf_just_loaded and "pdf_analyzed" not in st.session_state:
        analysis_msg = """
Harika! ğŸ‰ PDF dosyanÄ± analiz ettim ve hazÄ±rÄ±m.

ArtÄ±k belgenin iÃ§eriÄŸi hakkÄ±nda bana istediÄŸin sorularÄ± sorabilirsin. Ben sana en doÄŸru cevaplarÄ± vereceÄŸim! 

Ne Ã¶ÄŸrenmek istersin? ğŸ’¬
        """
        st.session_state.chat_history.append(("assistant", analysis_msg.strip()))
        st.session_state.pdf_analyzed = True

    # GeÃ§miÅŸ mesajlarÄ± gÃ¶ster
    for role, msg in st.session_state.chat_history:
        with st.chat_message(role):
            st.markdown(msg)

    # PDF yÃ¼klenmemiÅŸse input gÃ¶sterme
    if llm is None:
        return

    # KullanÄ±cÄ± sorusu
    soru = st.chat_input("Bir ÅŸey sor...")

    if not soru:
        return

    # KullanÄ±cÄ± mesajÄ±nÄ± kaydet ve gÃ¶ster
    st.session_state.chat_history.append(("user", soru))
    with st.chat_message("user"):
        st.markdown(soru)

    # Context al
    context = retriever(soru)

    # Chain oluÅŸtur
    chain = QA_PROMPT | llm

    # Asistan cevabÄ±
    with st.chat_message("assistant"):
        # Thinking animasyonu
        thinking_placeholder = st.empty()
        thinking_placeholder.markdown(
            """
            <div class="thinking-container">
                <span class="thinking-text">DÃ¼ÅŸÃ¼nÃ¼yor</span>
                <div class="thinking-dots">
                    <div class="thinking-dot"></div>
                    <div class="thinking-dot"></div>
                    <div class="thinking-dot"></div>
                </div>
            </div>
            """,
            unsafe_allow_html=True
        )

        # Cevap iÃ§in placeholder
        answer_placeholder = st.empty()
        full_answer = ""
        thinking_cleared = False

        # Stream cevap
        for chunk in chain.stream({
            "context": context,
            "question": soru,
            "chat_history": st.session_state.chat_history[-6:]
        }):
            if hasattr(chunk, "content") and chunk.content:
                # Ä°lk chunk geldiÄŸinde thinking'i kaldÄ±r
                if not thinking_cleared:
                    thinking_placeholder.empty()
                    thinking_cleared = True

                full_answer += chunk.content
                answer_placeholder.markdown(full_answer)

    # CevabÄ± geÃ§miÅŸe ekle
    st.session_state.chat_history.append(("assistant", full_answer))