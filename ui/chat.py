import streamlit as st
from prompts.qa_prompt import QA_PROMPT


def render_chat(llm, retriever):
    """Chat arayÃ¼zÃ¼nÃ¼ render eder"""
    
    # Chat history baÅŸlat
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    
    # Mesaj render edilme kontrolÃ¼ iÃ§in flag
    if "rendering" not in st.session_state:
        st.session_state.rendering = False
    
    # Ä°LK AÃ‡ILIÅ - PDF yÃ¼klenmemiÅŸ
    if llm is None:
        if len(st.session_state.chat_history) == 0:
            welcome_msg = """
Merhaba! ğŸ‘‹ Ben senin PDF asistanÄ±nÄ±m.

BaÅŸlamak iÃ§in soldaki menÃ¼den bir PDF dosyasÄ± yÃ¼kle, ben onu analiz edeyim. Sonra iÃ§eriÄŸi hakkÄ±nda istediÄŸin sorularÄ± sorabilirsin! 

Ne tÃ¼r sorular sorabileceÄŸini merak ediyorsan:
- ğŸ“„ "Bu belge ne hakkÄ±nda?"
- ğŸ” "X konusuyla ilgili ne diyor?"
- ğŸ“Š "Ã–nemli noktalarÄ± Ã¶zetle"
- ğŸ’¡ "Y hakkÄ±nda detaylÄ± bilgi ver"

Hadi, PDF'ini yÃ¼kle ve baÅŸlayalÄ±m! ğŸš€
            """
            st.session_state.chat_history.append(("assistant", welcome_msg.strip()))
        
        # KarÅŸÄ±lama mesajÄ±nÄ± gÃ¶ster
        with st.chat_message("assistant"):
            st.markdown(st.session_state.chat_history[0][1])
        
        return

    # PDF YÃœKLENMÄ°Å - Normal chat akÄ±ÅŸÄ±
    
    # GeÃ§miÅŸ mesajlarÄ± gÃ¶ster (sadece render edilmemiÅŸse)
    if not st.session_state.rendering:
        for role, msg in st.session_state.chat_history:
            with st.chat_message(role):
                st.markdown(msg)

    # KullanÄ±cÄ± sorusu
    soru = st.chat_input("Bir ÅŸey sor...")

    if not soru:
        return

    # Render flag'i aktif et
    st.session_state.rendering = True
    
    # KullanÄ±cÄ± mesajÄ±nÄ± kaydet
    st.session_state.chat_history.append(("user", soru))
    
    # KullanÄ±cÄ± mesajÄ±nÄ± gÃ¶ster
    with st.chat_message("user"):
        st.markdown(soru)

    # Context al
    context = retriever(soru)

    # Chain oluÅŸtur
    chain = QA_PROMPT | llm

    # Asistan cevabÄ±
    with st.chat_message("assistant"):
        # Thinking animasyonu
        thinking_placeholder = st.empty()
        thinking_placeholder.markdown(
            """
            <div class="thinking-container">
                <span class="thinking-text">DÃ¼ÅŸÃ¼nÃ¼yor</span>
                <div class="thinking-dots">
                    <div class="thinking-dot"></div>
                    <div class="thinking-dot"></div>
                    <div class="thinking-dot"></div>
                </div>
            </div>
            """,
            unsafe_allow_html=True
        )

        # Cevap iÃ§in placeholder
        answer_placeholder = st.empty()
        full_answer = ""
        thinking_cleared = False

        # Stream cevap
        try:
            for chunk in chain.stream({
                "context": context,
                "question": soru,
                "chat_history": st.session_state.chat_history[-6:]
            }):
                if hasattr(chunk, "content") and chunk.content:
                    # Ä°lk chunk geldiÄŸinde thinking'i kaldÄ±r
                    if not thinking_cleared:
                        thinking_placeholder.empty()
                        thinking_cleared = True

                    full_answer += chunk.content
                    answer_placeholder.markdown(full_answer)
                    
        except Exception as e:
            thinking_placeholder.empty()
            answer_placeholder.error(f"Bir hata oluÅŸtu: {str(e)}")
            st.session_state.rendering = False
            return

    # CevabÄ± kaydet
    st.session_state.chat_history.append(("assistant", full_answer))
    
    # Render flag'i kapat
    st.session_state.rendering = False